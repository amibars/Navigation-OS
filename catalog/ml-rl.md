# ML/RL

> Generated from README.md on 2026-01-29
## Index
- [deepscaler](#deepscaler)
- [rllm](#rllm)

---

## deepscaler

**TL;DR:** rLLM ‚Äî –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ reinforcement learning. –ü–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –æ–±—É—á–∞—Ç—å –∏—Ö —á–µ—Ä–µ–∑ RL –∏ –¥–µ–ø–ª–æ–∏—Ç—å. DeepScaleR-1.5B –ø—Ä–µ–≤–∑–æ—à—ë–ª O1-Preview –Ω–∞ AIME (43.1%).

### –ë—ã—Å—Ç—Ä—ã–π –≤—ã–±–æ—Ä
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏:
  - RL training for LLMs, O1 reproduction
  - DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL
  - DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level
- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏:
  - Production inference only
  - –ù—É–∂–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ/–¥–æ—Å—Ç—É–ø—ã: model, dataset

### üöÄ –ó–∞–ø—É—Å–∫
```bash
uv pip install rllm
```

### üß© –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- **Category:** ML/RL
- **Stack:** Python, Docker
- **Entrypoints:** –°–º. README

### üß™ –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á
- RL training for LLMs, O1 reproduction
- DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL
- DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level
- DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL

### ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- Production inference only
- –ù—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ/–¥–æ—Å—Ç—É–ø—ã: model, dataset

### üß≠ Fit / Maturity / Ops
- **Fit:** RL training for LLMs, O1 reproduction
- **Maturity:** active
- **Latency/Cost:** quality
- **Data constraints:** model, dataset
- **Ops friction:** low

### Full links
- Repo: https://github.com/amibars/deepscaler
- Original README: https://github.com/agentica-project/deepscaler/blob/main/README.md
- Docs: https://rllm-project.readthedocs.io
- Discord: https://discord.gg/BDH46HT9en

---



## rllm

**TL;DR:** RLLM (RL for LLMs). High-performance library for training LLMs with Reinforcement Learning (PPO, GRPO). Optimized for massive scale using vLLM backend. Democratizing DeepSeek-R1 style training pipelines. 5k stars.

### –ë—ã—Å—Ç—Ä—ã–π –≤—ã–±–æ—Ä
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏:
  - RL training for LLMs
  - DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL
  - DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level
- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏:
  - Inference only
  - –ù—É–∂–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ/–¥–æ—Å—Ç—É–ø—ã: GPU

### üöÄ –ó–∞–ø—É—Å–∫
```bash
pip install rllm
```

### üß© –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- **Category:** ML/RL
- **Stack:** Python, Docker
- **Entrypoints:** –°–º. README

### üß™ –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á
- RL training for LLMs
- DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL
- DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level
- DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL

### ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- Inference only
- –ù—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ/–¥–æ—Å—Ç—É–ø—ã: GPU

### üß≠ Fit / Maturity / Ops
- **Fit:** RL training for LLMs
- **Maturity:** active
- **Latency/Cost:** quality
- **Data constraints:** GPU
- **Ops friction:** low

### Full links
- Repo: https://github.com/rllm-org/rllm
- Original README: https://github.com/rllm-org/rllm/blob/main/README.md
- Stars: 5,033
- Maturity: active

---
